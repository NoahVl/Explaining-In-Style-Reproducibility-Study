{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from resnet_classifier import load_resnet_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "#model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True).to(device)\n",
    "#model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=False).to(device)\n",
    "#model.classifier[1] = nn.Linear(1280, 2).to(device)\n",
    "\n",
    "#model = load_classifier(\"FFHQ-Gender_res64.pth\", 0, 2)\n",
    "#model = load_classifier(\"CelebA-64-nodataaug.pt\", 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\noahv/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "Using cache found in C:\\Users\\noahv/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True).to(device)\n",
    "model.fc = nn.Linear(512, 2).to(device)\n",
    "\n",
    "model = load_resnet_classifier(\"resnet-18-64px-gender.pt\", 0, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=2, bias=True)\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from torchvision.transforms import InterpolationMode\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as vision_F\n",
    "\n",
    "class FFHQ(data.Dataset):\n",
    "    def __init__(self, ffhq_dir, csv_path, image_size=32, transform=None, label=\"gender\"):\n",
    "        \"\"\"\n",
    "        PyTorch DataSet for the FFHQ-Age dataset.\n",
    "        :param root: Root folder that contains a directory for the dataset and the csv with labels in the root directory.\n",
    "        :param label: Label we want to train on, chosen from the csv labels list.\n",
    "        \"\"\"\n",
    "        self.target_class = label\n",
    "\n",
    "        # Store image paths\n",
    "        self.images = [os.path.join(ffhq_dir, file)\n",
    "                       for file in os.listdir(ffhq_dir) if file.endswith('.jpg')]\n",
    "\n",
    "        # Import labels from a CSV file\n",
    "        self.labels = pd.read_csv(csv_path)\n",
    "\n",
    "        def transform_with_resize(tensor_images):\n",
    "            return vision_F.resize(tensor_images, [224, 224])\n",
    "\n",
    "        # Image transformation\n",
    "        self.transform = transform\n",
    "        if self.transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize(image_size),\n",
    "                #transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                #transforms.Resize(224),\n",
    "                transforms.Lambda(lambda x: transform_with_resize(x)),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "        # Make a lookup dictionary for the labels\n",
    "        # Get column names of dataframe\n",
    "        cols = self.labels.columns.values\n",
    "        label_ids = {col_name: i for i, col_name in enumerate(cols)}\n",
    "        self.class_id = label_ids[self.target_class]\n",
    "\n",
    "        self.one_hot_encoding = {\"male\": 0,\n",
    "                                 \"female\": 1}\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        _img = self.transform(Image.open(self.images[index]))\n",
    "        _label = self.one_hot_encoding[self.labels.iloc[index, self.class_id]]\n",
    "        return _img, _label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "\n",
    "class CelebA(data.Dataset):\n",
    "    def __init__(self, celeb_dir, csv_path, image_size=32, transform=None, label=\"Male\"):\n",
    "        \"\"\"\n",
    "        PyTorch DataSet for the FFHQ-Age dataset.\n",
    "        :param root: Root folder that contains a directory for the dataset and the csv with labels in the root directory.\n",
    "        :param label: Label we want to train on, chosen from the csv labels list.\n",
    "        \"\"\"\n",
    "        self.target_class = label\n",
    "\n",
    "        # Store image paths\n",
    "        image_path = os.path.join(celeb_dir, \"img_align_celeba\", \"img_align_celeba\")\n",
    "        self.images = [os.path.join(image_path, file)\n",
    "                       for file in os.listdir(image_path) if file.endswith('.jpg')]\n",
    "\n",
    "        # Import labels from a CSV file\n",
    "        self.labels = pd.read_csv(csv_path)\n",
    "\n",
    "        # Image transformation\n",
    "        self.transform = transform\n",
    "        if self.transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize(image_size),\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "        # Make a lookup dictionary for the labels\n",
    "        # Get column names of dataframe\n",
    "        cols = self.labels.columns.values\n",
    "        label_ids = {col_name: i for i, col_name in enumerate(cols)}\n",
    "        self.class_id = label_ids[self.target_class]\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        _img = self.transform(Image.open(self.images[index]))\n",
    "        _label = 0 if self.labels.iloc[index, self.class_id] == 1 else 1  # Male will be the first number as with FFHQ upstairs\n",
    "        return _img, _label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def get_train_valid_test_dataset(celeba_dir, csv_path, label, image_size=32, valid_ratio=0.15, test_ratio=0.15):\n",
    "    # TODO: Specify different training routines here per class (such as random crop, random horizontal flip, etc.)\n",
    "\n",
    "    dataset = CelebA(celeba_dir, csv_path, image_size=image_size, label=label)\n",
    "    train_length, valid_length, test_length = int(len(dataset) * (1 - valid_ratio - test_ratio)), \\\n",
    "                                              int(len(dataset) * valid_ratio), int(len(dataset) * test_ratio)\n",
    "    # Make sure that the lengths sum to the total length of the dataset\n",
    "    remainder = len(dataset) - train_length - valid_length - test_length\n",
    "    train_length += remainder\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset,\n",
    "                                                                             [train_length, valid_length, test_length],\n",
    "                                                                             generator=torch.Generator().manual_seed(42)\n",
    "                                                                             )\n",
    "    \"\"\"\n",
    "    train_dataset.set_transform = A.Compose(\n",
    "        [\n",
    "            transforms.Resize(image_size),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "            A.HorizontalFlip(p=0.2),\n",
    "            A.RandomBrightnessContrast(p=0.3, brightness_limit=0.25, contrast_limit=0.5),\n",
    "            A.MotionBlur(p=.2),\n",
    "            A.GaussNoise(p=.2),\n",
    "            A.ImageCompression(p=.2, quality_lower=50),\n",
    "            A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "            transforms.Resize(224),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "celeb_train, celeb_val, celeb_test = get_train_valid_test_dataset(\"../data/CelebA/celeba-dataset/\", \"../data/CelebA/celeba-dataset/list_attr_celeba.csv\", \"Male\", image_size=64)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "batch_size = 128\n",
    "cel_train_loader = DataLoader(celeb_train, batch_size=batch_size, pin_memory=True)\n",
    "cel_val_loader = DataLoader(celeb_val, batch_size=batch_size)\n",
    "cel_test_loader = DataLoader(celeb_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import notebook\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model.requires_grad_(False)\n",
    "model.fc.requires_grad_(True)\n",
    "\n",
    "def validate_model(model, loader, criterion):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "\n",
    "    # Set the model to evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize the loss and accuracy.\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    # For each batch in the validation set...\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(notebook.tqdm_notebook(loader)):\n",
    "            # Send the batch to the device.\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Forward pass.\n",
    "            output = model(data)\n",
    "\n",
    "            # Calculate the loss.\n",
    "            loss += criterion(output, target).item() * len(target)/128\n",
    "\n",
    "            # Get the predictions.\n",
    "            preds = torch.argmax(output, 1)\n",
    "\n",
    "            # Calculate the accuracy.\n",
    "            accuracy += torch.sum(preds == target).item() * len(target)/128\n",
    "\n",
    "    # Calculate the average loss and accuracy.\n",
    "    loss /= len(loader)\n",
    "    accuracy /= len(loader) * batch_size\n",
    "\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, test_model=False, epochs=10):\n",
    "    \"\"\"Trains model\"\"\"\n",
    "\n",
    "    # Put the model in training mode.\n",
    "    model.train()\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch in range(epochs):\n",
    "        # For each batch in the training set...\n",
    "        for batch_idx, (data, target) in enumerate(notebook.tqdm_notebook(train_loader)):\n",
    "            # Send the data and labels to the device.\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Zero out the gradients.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass.\n",
    "            output = model(data)\n",
    "\n",
    "            # Calculate the loss.\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Backward pass.\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print the loss.\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Epoch: {}/{}'.format(epoch + 1, epochs),\n",
    "                      'Loss: {:.4f}'.format(loss.item()))\n",
    "\n",
    "        # Validate the model.\n",
    "        val_loss, val_acc = validate_model(model, val_loader, criterion)\n",
    "\n",
    "        # Print the validation loss.\n",
    "        print('Validation Loss: {:.4f}'.format(val_loss))\n",
    "\n",
    "        # Print the validation accuracy.\n",
    "        print('Validation Accuracy: {:.4f}'.format(val_acc))\n",
    "\n",
    "    if test_model:\n",
    "        # Test the model.\n",
    "        test_loss, test_acc = validate_model(model, test_loader, criterion)\n",
    "\n",
    "        # Print the test loss.\n",
    "        print('Test Loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "        # Print the test accuracy.\n",
    "        print('Test Accuracy: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/238 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6a94dc6e3564d0b9b25b007bf82626d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0626\n",
      "Validation Accuracy: 0.9739\n"
     ]
    }
   ],
   "source": [
    "# Validate the model.\n",
    "val_loss, val_acc = validate_model(model, cel_val_loader, criterion)\n",
    "\n",
    "# Print the validation loss.\n",
    "print('Validation Loss: {:.4f}'.format(val_loss))\n",
    "\n",
    "# Print the validation accuracy.\n",
    "print('Validation Accuracy: {:.4f}'.format(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/238 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94e37c23627c42db9cbc54e40c0c6f85"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0601\n",
      "Test Accuracy: 0.9743\n"
     ]
    }
   ],
   "source": [
    "# Test the model.\n",
    "test_loss, test_acc = validate_model(model, cel_test_loader, criterion)\n",
    "\n",
    "# Print the test loss.\n",
    "print('Test Loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "# Print the test accuracy.\n",
    "print('Test Accuracy: {:.4f}'.format(test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing on FFHQ"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def get_train_valid_test_dataset(ffhq_dir, csv_path, label, image_size=32, valid_ratio=0.15, test_ratio=0.15):\n",
    "    # TODO: Specify different training routines here per class (such as random crop, random horizontal flip, etc.)\n",
    "\n",
    "    dataset = FFHQ(ffhq_dir, csv_path, image_size=image_size, label=label)\n",
    "    train_length, valid_length, test_length = int(len(dataset) * (1 - valid_ratio - test_ratio)), \\\n",
    "                                              int(len(dataset) * valid_ratio), int(len(dataset) * test_ratio)\n",
    "    # Make sure that the lengths sum to the total length of the dataset\n",
    "    remainder = len(dataset) - train_length - valid_length - test_length\n",
    "    train_length += remainder\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset,\n",
    "                                                                             [train_length, valid_length, test_length],\n",
    "                                                                             generator=torch.Generator().manual_seed(42)\n",
    "                                                                             )\n",
    "    \"\"\"\n",
    "    train_dataset.set_transform = A.Compose(\n",
    "        [\n",
    "            transforms.Resize(image_size),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "            A.HorizontalFlip(p=0.2),\n",
    "            A.RandomBrightnessContrast(p=0.3, brightness_limit=0.25, contrast_limit=0.5),\n",
    "            A.MotionBlur(p=.2),\n",
    "            A.GaussNoise(p=.2),\n",
    "            A.ImageCompression(p=.2, quality_lower=50),\n",
    "            A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "            transforms.Resize(224),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "train, val, test = get_train_valid_test_dataset(\"../data/Kaggle_FFHQ_Resized_256px/flickrfaceshq-dataset-nvidia-resized-256px/resized\", \"../data/Kaggle_FFHQ_Resized_256px/ffhq_aging_labels.csv\", \"gender\", image_size=64)\n",
    "train_loader = DataLoader(train, batch_size=128)\n",
    "val_loader = DataLoader(val, batch_size=128)\n",
    "test_loader = DataLoader(test, batch_size=128)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/383 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "098f750a3b57493685ed7e71e0ec303f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import notebook\n",
    "\n",
    "y_preds = []\n",
    "y_trues = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, target) in enumerate(notebook.tqdm_notebook(train_loader)):\n",
    "        y_trues.append(target.cpu())\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "\n",
    "        preds = torch.argmax(output, 1)\n",
    "\n",
    "        y_preds.append(preds.cpu())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/83 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "de442caa906c48298d008b6d54e6443b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import notebook\n",
    "\n",
    "#y_preds = []\n",
    "#y_trues = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, target) in enumerate(notebook.tqdm_notebook(val_loader)):\n",
    "        y_trues.append(target.cpu())\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "\n",
    "        preds = torch.argmax(output, 1)\n",
    "\n",
    "        y_preds.append(preds.cpu())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/83 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b396bde1eb5340a68a36a622a8717e46"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import notebook\n",
    "\n",
    "#y_preds = []\n",
    "#y_trues = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, target) in enumerate(notebook.tqdm_notebook(test_loader)):\n",
    "        y_trues.append(target.cpu())\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "\n",
    "        preds = torch.argmax(output, 1)\n",
    "\n",
    "        y_preds.append(preds.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_preds = np.concatenate(y_preds)\n",
    "y_trues = np.concatenate(y_trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "C = confusion_matrix(y_trues, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[26154,  6016],\n       [ 2541, 35289]], dtype=int64)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Entire dataset:\")\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8777571428571429"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_trues, y_preds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cfa3962ee0560444ad985082d3a9d1d3cf5b3a106c0ad670dbb0d52cc4dc0741"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}