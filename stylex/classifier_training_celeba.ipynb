{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from resnet_classifier import load_resnet_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "#model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True).to(device)\n",
    "#model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=False).to(device)\n",
    "#model.classifier[1] = nn.Linear(1280, 2).to(device)\n",
    "\n",
    "#model = load_classifier(\"FFHQ-Gender_res64.pth\", 0, 2)\n",
    "#model = load_classifier(\"CelebA-64-nodataaug.pt\", 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\noahv/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True).to(device)\n",
    "model.fc = nn.Linear(512, 2).to(device)\n",
    "\n",
    "#model = load_resnet_classifier(\"resnet-18-64px-unfreezel4.pt\", 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=2, bias=True)\n)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import InterpolationMode\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as vision_F\n",
    "\n",
    "class FFHQ(data.Dataset):\n",
    "    def __init__(self, ffhq_dir, csv_path, image_size=32, transform=None, label=\"gender\"):\n",
    "        \"\"\"\n",
    "        PyTorch DataSet for the FFHQ-Age dataset.\n",
    "        :param root: Root folder that contains a directory for the dataset and the csv with labels in the root directory.\n",
    "        :param label: Label we want to train on, chosen from the csv labels list.\n",
    "        \"\"\"\n",
    "        self.target_class = label\n",
    "\n",
    "        # Store image paths\n",
    "        self.images = [os.path.join(ffhq_dir, file)\n",
    "                       for file in os.listdir(ffhq_dir) if file.endswith('.jpg')]\n",
    "\n",
    "        # Import labels from a CSV file\n",
    "        self.labels = pd.read_csv(csv_path)\n",
    "\n",
    "        def transform_with_resize(tensor_images):\n",
    "            return vision_F.resize(tensor_images, [224, 224])\n",
    "\n",
    "        # Image transformation\n",
    "        self.transform = transform\n",
    "        if self.transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize(image_size),\n",
    "                #transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                #transforms.Resize(224),\n",
    "                transforms.Lambda(lambda x: transform_with_resize(x)),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "        # Make a lookup dictionary for the labels\n",
    "        # Get column names of dataframe\n",
    "        cols = self.labels.columns.values\n",
    "        label_ids = {col_name: i for i, col_name in enumerate(cols)}\n",
    "        self.class_id = label_ids[self.target_class]\n",
    "\n",
    "        self.one_hot_encoding = {\"male\": 0,\n",
    "                                 \"female\": 1}\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        _img = self.transform(Image.open(self.images[index]))\n",
    "        _label = self.one_hot_encoding[self.labels.iloc[index, self.class_id]]\n",
    "        return _img, _label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "\n",
    "class CelebA(data.Dataset):\n",
    "    def __init__(self, celeb_dir, csv_path, image_size=32, transform=None, label=\"Male\"):\n",
    "        \"\"\"\n",
    "        PyTorch DataSet for the CelebA-Age dataset.\n",
    "        :param root: Root folder that contains a directory for the dataset and the csv with labels in the root directory.\n",
    "        :param label: Label we want to train on, chosen from the csv labels list.\n",
    "        \"\"\"\n",
    "        self.target_class = label\n",
    "\n",
    "        # Store image paths\n",
    "        image_path = os.path.join(celeb_dir, \"img_align_celeba\", \"img_align_celeba\")\n",
    "        self.images = [os.path.join(image_path, file)\n",
    "                       for file in os.listdir(image_path) if file.endswith('.jpg')]\n",
    "\n",
    "        # Import labels from a CSV file\n",
    "        self.labels = pd.read_csv(csv_path)\n",
    "\n",
    "        # Image transformation\n",
    "        self.transform = transform\n",
    "        if self.transform is None:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize(image_size),\n",
    "                transforms.Resize(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "        # Make a lookup dictionary for the labels\n",
    "        # Get column names of dataframe\n",
    "        cols = self.labels.columns.values\n",
    "        label_ids = {col_name: i for i, col_name in enumerate(cols)}\n",
    "        self.class_id = label_ids[self.target_class]\n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        _img = self.transform(Image.open(self.images[index]))\n",
    "        _label = 0 if self.labels.iloc[index, self.class_id] == 1 else 1  # Male will be the first number as with FFHQ upstairs\n",
    "        return _img, _label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_valid_test_dataset(celeba_dir, csv_path, label, image_size=32, valid_ratio=0.15, test_ratio=0.15):\n",
    "    # TODO: Specify different training routines here per class (such as random crop, random horizontal flip, etc.)\n",
    "\n",
    "    dataset = CelebA(celeba_dir, csv_path, image_size=image_size, label=label)\n",
    "    train_length, valid_length, test_length = int(len(dataset) * (1 - valid_ratio - test_ratio)), \\\n",
    "                                              int(len(dataset) * valid_ratio), int(len(dataset) * test_ratio)\n",
    "    # Make sure that the lengths sum to the total length of the dataset\n",
    "    remainder = len(dataset) - train_length - valid_length - test_length\n",
    "    train_length += remainder\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset,\n",
    "                                                                             [train_length, valid_length, test_length],\n",
    "                                                                             generator=torch.Generator().manual_seed(42)\n",
    "                                                                             )\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Male for gender\n",
    "# Young for age\n",
    "celeb_train, celeb_val, celeb_test = get_train_valid_test_dataset(\"../data/CelebA/celeba-dataset/\", \"../data/CelebA/celeba-dataset/list_attr_celeba.csv\", \"Male\", image_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "batch_size = 128\n",
    "cel_train_loader = DataLoader(celeb_train, batch_size=batch_size, pin_memory=True)\n",
    "cel_val_loader = DataLoader(celeb_val, batch_size=batch_size)\n",
    "cel_test_loader = DataLoader(celeb_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import notebook\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def validate_model(model, loader, criterion):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "\n",
    "    # Set the model to evaluation mode.\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize the loss and accuracy.\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    # For each batch in the validation set...\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(notebook.tqdm_notebook(loader)):\n",
    "            # Send the batch to the device.\n",
    "\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Forward pass.\n",
    "            output = model(data)\n",
    "\n",
    "            # Calculate the loss.\n",
    "            loss += criterion(output, target).item() * len(target)/128\n",
    "\n",
    "            # Get the predictions.\n",
    "            preds = torch.argmax(output, 1)\n",
    "\n",
    "            # Calculate the accuracy.\n",
    "            accuracy += torch.sum(preds == target).item() * len(target)/128\n",
    "\n",
    "    # Calculate the average loss and accuracy.\n",
    "    loss /= len(loader)\n",
    "    accuracy /= len(loader) * batch_size\n",
    "\n",
    "    return loss, accuracy\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, test_model=False, epochs=10):\n",
    "    \"\"\"Trains model\"\"\"\n",
    "\n",
    "    # Put the model in training mode.\n",
    "    model.train()\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch in range(epochs):\n",
    "        # For each batch in the training set...\n",
    "        for batch_idx, (data, target) in enumerate(notebook.tqdm_notebook(train_loader)):\n",
    "            # Send the data and labels to the device.\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Zero out the gradients.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass.\n",
    "            output = model(data)\n",
    "\n",
    "            # Calculate the loss.\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Backward pass.\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print the loss.\n",
    "            if batch_idx % 100 == 0:\n",
    "                print('Epoch: {}/{}'.format(epoch + 1, epochs),\n",
    "                      'Loss: {:.4f}'.format(loss.item()))\n",
    "\n",
    "        # Validate the model.\n",
    "        val_loss, val_acc = validate_model(model, val_loader, criterion)\n",
    "\n",
    "        # Print the validation loss.\n",
    "        print('Validation Loss: {:.4f}'.format(val_loss))\n",
    "\n",
    "        # Print the validation accuracy.\n",
    "        print('Validation Accuracy: {:.4f}'.format(val_acc))\n",
    "\n",
    "    if test_model:\n",
    "        # Test the model.\n",
    "        test_loss, test_acc = validate_model(model, test_loader, criterion)\n",
    "\n",
    "        # Print the test loss.\n",
    "        print('Test Loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "        # Print the test accuracy.\n",
    "        print('Test Accuracy: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Only run for ResNet training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1108 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "967c8694d23e4a78b52083e84775084d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1 Loss: 0.6519\n",
      "Epoch: 1/1 Loss: 0.4630\n",
      "Epoch: 1/1 Loss: 0.3968\n",
      "Epoch: 1/1 Loss: 0.3988\n",
      "Epoch: 1/1 Loss: 0.2796\n",
      "Epoch: 1/1 Loss: 0.3115\n",
      "Epoch: 1/1 Loss: 0.4078\n",
      "Epoch: 1/1 Loss: 0.2540\n",
      "Epoch: 1/1 Loss: 0.2625\n",
      "Epoch: 1/1 Loss: 0.2948\n",
      "Epoch: 1/1 Loss: 0.3002\n",
      "Epoch: 1/1 Loss: 0.2826\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/238 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c09c1d8cfcde48e78e3d59d7babe828b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2618\n",
      "Validation Accuracy: 0.8936\n"
     ]
    }
   ],
   "source": [
    "# Only unfreeze last layer.\n",
    "model.requires_grad_(False)\n",
    "model.fc.requires_grad_(True)\n",
    "train_model(model, cel_train_loader, cel_val_loader, optimizer, criterion, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1108 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1060e7beaee94c50bca2e5d8057abdb6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1 Loss: 0.2762\n",
      "Epoch: 1/1 Loss: 0.0631\n",
      "Epoch: 1/1 Loss: 0.0379\n",
      "Epoch: 1/1 Loss: 0.0834\n",
      "Epoch: 1/1 Loss: 0.0525\n",
      "Epoch: 1/1 Loss: 0.2094\n",
      "Epoch: 1/1 Loss: 0.0832\n",
      "Epoch: 1/1 Loss: 0.0308\n",
      "Epoch: 1/1 Loss: 0.0570\n",
      "Epoch: 1/1 Loss: 0.0449\n",
      "Epoch: 1/1 Loss: 0.0995\n",
      "Epoch: 1/1 Loss: 0.0916\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/238 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d180205f8a4948b68d294fca3b797810"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0629\n",
      "Validation Accuracy: 0.9734\n"
     ]
    }
   ],
   "source": [
    "# Also unfreeze second to last layer.\n",
    "model.layer4.requires_grad_(True)\n",
    "train_model(model, cel_train_loader, cel_val_loader, optimizer, criterion, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1108 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4b51f94bd2aa435c981b5aa964e29d81"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1 Loss: 0.0427\n",
      "Epoch: 1/1 Loss: 0.0369\n",
      "Epoch: 1/1 Loss: 0.0034\n",
      "Epoch: 1/1 Loss: 0.0144\n",
      "Epoch: 1/1 Loss: 0.0098\n",
      "Epoch: 1/1 Loss: 0.1176\n",
      "Epoch: 1/1 Loss: 0.0400\n",
      "Epoch: 1/1 Loss: 0.0115\n",
      "Epoch: 1/1 Loss: 0.0286\n",
      "Epoch: 1/1 Loss: 0.0440\n",
      "Epoch: 1/1 Loss: 0.0239\n",
      "Epoch: 1/1 Loss: 0.0151\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/238 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "189000af380843bc9c492af4996c00f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0752\n",
      "Validation Accuracy: 0.9712\n"
     ]
    }
   ],
   "source": [
    "# Also unfreeze third to last layer.\n",
    "model.layer3.requires_grad_(True)\n",
    "train_model(model, cel_train_loader, cel_val_loader, optimizer, criterion, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Only run when you don't plan to make changes to the model (training)!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test the model.\n",
    "test_loss, test_acc = validate_model(model, cel_test_loader, criterion)\n",
    "\n",
    "# Print the test loss.\n",
    "print('Test Loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "# Print the test accuracy.\n",
    "print('Test Accuracy: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Only for Resnet, train with all layers unfrozen\n",
    "#import gc\n",
    "#torch.cuda.empty_cache()\n",
    "#gc.collect()\n",
    "#model.requires_grad_(True)\n",
    "#train_model(model, train_loader, val_loader, optimizer, criterion, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save model state dict to file\n",
    "\n",
    "# Trained on 256 works better than 128 for 128\n",
    "torch.save(model.state_dict(), './resnet-18-64px-gender-classifier.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Only run this for MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Freeze/unfreeze layers after converging with training\n",
    "# Mobilenet training\n",
    "\n",
    "# Trained with only classifier unfrozen\n",
    "#model.features.requires_grad_(True)\n",
    "#model.classifier.requires_grad_(True)\n",
    "\n",
    "#model.features[0:15].requires_grad_(False)\n",
    "#model.features[15:].requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Trained with layers until 15 frozen\n",
    "# train_model(model, train_loader, val_loader, optimizer, criterion, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Freeze/unfreeze layers after converging with training\n",
    "\n",
    "#model.features[0:13].requires_grad_(False)\n",
    "#model.features[13:].requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Trained with layers until 13 frozen\n",
    "# train_model(model, train_loader, val_loader, optimizer, criterion, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test the model.\n",
    "#test_loss, test_acc = validate_model(model, test_loader, criterion)\n",
    "\n",
    "# Print the test loss.\n",
    "#print('Test Loss: {:.4f}'.format(test_loss))\n",
    "\n",
    "# Print the test accuracy.\n",
    "#print('Test Accuracy: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#_train_loader = DataLoader(train, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run this to test the model on FFHQ (not necessarily ground truth labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def get_train_valid_test_dataset(ffhq_dir, csv_path, label, image_size=32, valid_ratio=0.15, test_ratio=0.15):\n",
    "    # TODO: Specify different training routines here per class (such as random crop, random horizontal flip, etc.)\n",
    "\n",
    "    dataset = FFHQ(ffhq_dir, csv_path, image_size=image_size, label=label)\n",
    "    train_length, valid_length, test_length = int(len(dataset) * (1 - valid_ratio - test_ratio)), \\\n",
    "                                              int(len(dataset) * valid_ratio), int(len(dataset) * test_ratio)\n",
    "    # Make sure that the lengths sum to the total length of the dataset\n",
    "    remainder = len(dataset) - train_length - valid_length - test_length\n",
    "    train_length += remainder\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset,\n",
    "                                                                             [train_length, valid_length, test_length],\n",
    "                                                                             generator=torch.Generator().manual_seed(42)\n",
    "                                                                             )\n",
    "    \"\"\"\n",
    "    train_dataset.set_transform = A.Compose(\n",
    "        [\n",
    "            transforms.Resize(image_size),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "            A.HorizontalFlip(p=0.2),\n",
    "            A.RandomBrightnessContrast(p=0.3, brightness_limit=0.25, contrast_limit=0.5),\n",
    "            A.MotionBlur(p=.2),\n",
    "            A.GaussNoise(p=.2),\n",
    "            A.ImageCompression(p=.2, quality_lower=50),\n",
    "            A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "            transforms.Resize(224),\n",
    "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train, val, test = get_train_valid_test_dataset(\"../data/Kaggle_FFHQ_Resized_256px/flickrfaceshq-dataset-nvidia-resized-256px/resized\", \"../data/Kaggle_FFHQ_Resized_256px/ffhq_aging_labels.csv\", \"gender\", image_size=64)\n",
    "train_loader = DataLoader(train, batch_size=128)\n",
    "val_loader = DataLoader(val, batch_size=128)\n",
    "test_loader = DataLoader(test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/383 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "44065335364e4ca4b97511c96e3bdc0a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import notebook\n",
    "\n",
    "y_preds = []\n",
    "y_trues = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, target) in enumerate(notebook.tqdm_notebook(train_loader)):\n",
    "        y_trues.append(target.cpu())\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "\n",
    "        preds = torch.argmax(output, 1)\n",
    "\n",
    "        y_preds.append(preds.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/83 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bef0af75ba2744db94c8858fc94c7fc3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import notebook\n",
    "\n",
    "#y_preds = []\n",
    "#y_trues = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, target) in enumerate(notebook.tqdm_notebook(val_loader)):\n",
    "        y_trues.append(target.cpu())\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "\n",
    "        preds = torch.argmax(output, 1)\n",
    "\n",
    "        y_preds.append(preds.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/83 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "33e6b43fd4934bdd88c8a9c3e8247bf5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import notebook\n",
    "\n",
    "#y_preds = []\n",
    "#y_trues = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, target) in enumerate(notebook.tqdm_notebook(test_loader)):\n",
    "        y_trues.append(target.cpu())\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "\n",
    "        preds = torch.argmax(output, 1)\n",
    "\n",
    "        y_preds.append(preds.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_preds = np.concatenate(y_preds)\n",
    "y_trues = np.concatenate(y_trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "C = confusion_matrix(y_trues, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire dataset:\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[25269,  6901],\n       [ 1706, 36124]], dtype=int64)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Entire dataset:\")\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.8770428571428571"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_trues, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cfa3962ee0560444ad985082d3a9d1d3cf5b3a106c0ad670dbb0d52cc4dc0741"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}